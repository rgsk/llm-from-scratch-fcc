{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'mps' if torch.has_mps else 'cpu'\n",
    "print(device)\n",
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232309\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DOROTHY AND THE WIZARD IN OZ\n",
      "\n",
      "  BY\n",
      "\n",
      "  L. FRANK BAUM\n",
      "\n",
      "  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
      "\n",
      "  ILLUSTRATED BY JOHN R. NEILL\n",
      "\n",
      "  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n',\n",
      " ' ',\n",
      " '!',\n",
      " '\"',\n",
      " '&',\n",
      " \"'\",\n",
      " '(',\n",
      " ')',\n",
      " '*',\n",
      " ',',\n",
      " '-',\n",
      " '.',\n",
      " '0',\n",
      " '1',\n",
      " '2',\n",
      " '3',\n",
      " '4',\n",
      " '5',\n",
      " '6',\n",
      " '7',\n",
      " '8',\n",
      " '9',\n",
      " ':',\n",
      " ';',\n",
      " '?',\n",
      " 'A',\n",
      " 'B',\n",
      " 'C',\n",
      " 'D',\n",
      " 'E',\n",
      " 'F',\n",
      " 'G',\n",
      " 'H',\n",
      " 'I',\n",
      " 'J',\n",
      " 'K',\n",
      " 'L',\n",
      " 'M',\n",
      " 'N',\n",
      " 'O',\n",
      " 'P',\n",
      " 'Q',\n",
      " 'R',\n",
      " 'S',\n",
      " 'T',\n",
      " 'U',\n",
      " 'V',\n",
      " 'W',\n",
      " 'X',\n",
      " 'Y',\n",
      " 'Z',\n",
      " '[',\n",
      " ']',\n",
      " '_',\n",
      " 'a',\n",
      " 'b',\n",
      " 'c',\n",
      " 'd',\n",
      " 'e',\n",
      " 'f',\n",
      " 'g',\n",
      " 'h',\n",
      " 'i',\n",
      " 'j',\n",
      " 'k',\n",
      " 'l',\n",
      " 'm',\n",
      " 'n',\n",
      " 'o',\n",
      " 'p',\n",
      " 'q',\n",
      " 'r',\n",
      " 's',\n",
      " 't',\n",
      " 'u',\n",
      " 'v',\n",
      " 'w',\n",
      " 'x',\n",
      " 'y',\n",
      " 'z',\n",
      " '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "chars = sorted(set(text))\n",
    "pprint(chars)\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 58, 65, 65, 68]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode('hello'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character Level Tokenizer - \n",
    "converts each character to integer equivalents.\n",
    "\n",
    "in this case, we have very small vocabulary and very large amount of tokens to convert (large number of predictions would be required to generate the text)\n",
    "\n",
    "Word Level Tokenizer - \n",
    "we would map each word in a set of vocabulary to integers.\n",
    "\n",
    "in this case the vocabulary would be very large, but less number of tokens to encode/decode\n",
    "\n",
    "Sub-Word Tokenizer - \n",
    "in-between character and word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([80,  1,  1,  ..., 29, 67, 57])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
       "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
       "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
       "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
       "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
       "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 30097,  49156,  42546, 103302])\n",
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[71, 57, 58, 57,  1, 61, 58, 71],\n",
      "        [65, 72, 11,  0,  0, 33, 67, 72],\n",
      "        [ 1, 61, 62, 72,  1, 61, 54, 73],\n",
      "        [73, 71, 68, 73, 73, 58, 57,  1]], device='mps:0')\n",
      "targets: \n",
      "tensor([[57, 58, 57,  1, 61, 58, 71,  1],\n",
      "        [72, 11,  0,  0, 33, 67, 72, 62],\n",
      "        [61, 62, 72,  1, 61, 54, 73,  1],\n",
      "        [71, 68, 73, 73, 58, 57,  1, 68]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    print(ix)\n",
    "    x = torch.stack([data[i: i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs: ')\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "print('targets: ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32])\n",
      "when input is tensor([80]) target is tensor(1)\n",
      "when input is tensor([80,  1]) target is tensor(1)\n",
      "when input is tensor([80,  1,  1]) target is tensor(28)\n",
      "when input is tensor([80,  1,  1, 28]) target is tensor(39)\n",
      "when input is tensor([80,  1,  1, 28, 39]) target is tensor(42)\n",
      "when input is tensor([80,  1,  1, 28, 39, 42]) target is tensor(39)\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39]) target is tensor(44)\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39, 44]) target is tensor(32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_data[:block_size+1])\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print('when input is', context, 'target is', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam and AdamW are just some features that we add on to gradient descent\n",
    "# Adam - is a popular optimization algorithm that combines ideas of momentum. It uses a moving average of both the gradient and its squared value to adapt the learning rate of each parameter.\n",
    "# AdamW - is a modification of Adam optimizer and it adds weight decay. It generalizes the parameters more. So, instead of having very high level performance or very low level, it takes a little generalize in between.\n",
    "# the weight significance will actually shrink as it flattens out. Basically prevent over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1if*i]?sAlIk2hD&u9\n",
      "-)_oiz0;aj&X['52p.I&,K2zkiNvXwPgCnYqv56HJhuA0WO_vUx6gkT6EgQmOAMr[r4FamVzMg0cwnL;J-Qi'GpPPx0Sq5gwRDw-Kp P4]DGQ.,Byt)O'p.9d_G,QJLd,aQ3mQ&XwsjyKhK2vgZG!0XZDlO\"DG2JT6xwv?b2-&JD6pf2?\"&ctm2IBu-0gFN?157zZ 4\n",
      "I)LeZB'AZ\n",
      "QHI&r[1hr(*2:K2hcd8&eXh1-\n",
      "Net_oGzDGFql]pDG&e﻿v2?I!jd6﻿IE06ziMFN58LGLk8Rrw1UWrSlrB):_-K5[1MG[7(a];rSM*NqcBL﻿IMrsHlfcaCwn\n",
      "114\n",
      "u!s_okeclDG﻿V)-dY[RPYu'﻿57ADxZ2X;rv,3P1&mLY9Wob[LR2﻿U?SjyjUA;vU9﻿WF,nnq7a]* LQTVD7WF rd&fxvKHq[-'y ;4,2QZ5Jc_W-;1Y[O0W;Vsvp.]8!9nL,\n",
      "YZk?b﻿d﻿9 i-K;K\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    def forward(self, index, targets = None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
